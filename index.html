<!doctype html>
<head>
	<title>CS247 P3: Gesture Controller | Reno Bowen, Nicholas Hippenmeyer, Ilias Karim, Brad Lawson, & Keegan Poppen</title>

	<link rel="stylesheet" href="css/style.css">

	<link href='http://fonts.googleapis.com/css?family=GFS+Didot' rel='stylesheet' type='text/css'>

	<meta name="author" content="Reno Bowen, Nicholas Hippenmeyer, Ilias Karim, Brad Lawson, and Keegan Poppen">
	<meta name="description" content="">
	<meta name="keywords" content="">

	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1">

	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
</head>

<body>
	<header>
		<h1>
CS247 Project 3: Gesture Controller
		</h1>
		<h2>
Reno Bowen, Nicholas Hippenmeyer, Ilias Karim, Brad Lawson, and Keegan Poppen
		</h2>
		
		<nav>
			<a href="#introduction">
I. Introduction</a>
|
			<a href="#design_process">
II. Design Process</a>
|
			<a href="#implementation">
III. Implementation</a>
|
			<a href="#discussion">
IV. Discussion</a>
		</nav>
	</header>

	<div>

		<a name="introduction">
			<h1>
I. Introduction
			</h1>

			<p>
Designing gestures to select virtual targets via the sophisticated technologies embedded in the Kinect controller and its SDK presents unique problems that we would otherwise not encounter designing and developing interfaces for more conventional interface devices like mice, trackpads, or touchscreens. Best practices for designing such interfaces have yet to be established, and the inherent nature of the controller and its limitations demand a novel approach and paradigm for user interaction. Though daunting, we regard this challenge as an incredible opportunity to explore and hopefully influence how humans will one day interact with many of their devices--that is, through natural user interfaces, without the need to handle controllers physically.
			</p>
		</a>


		<a name="design_process">

			<h1>
II. Design Process
			</h1>

			<p>
Our group of five coalesced around the broad theme of music applications. At first we excitedly brainstormed, discussed, and illustrated use cases, user needs, and crazy solutions that addressed these use cases and user needs, ignoring Kinect's limitations. We asked ambitious, broad questions like: How could we augment the physical space of the room the Kinect and its user are in? How could we simultaneously engage and stimulate the user's attention and body? How can we reimagine the way the user interacts with and relates to his/her music? And we also asked simpler yet equally important questions like: what are unmistakable gestures that are not commonly acted out but are easily detected?
			</p>

			<img src="img/notes1.jpg">
			<span class="caption">
caption
			</span>

			<img src="img/whiteboard.jpg">
			<span class="caption">
caption
			</span>

			<img src="img/notes2.jpg">
			<span class="caption">
caption
			</span>

			<p>
To refine our ideas, we reconsidered them in light of some of the Kinect's limitations, including the fact that it can only actively track the joints of two users and that there was a certain sweet spot for tracking users somewhere between 6' and 9' from the device.  One of the greatest challenges we faced in designing gestures was to design them in a manner that made triggering them accidentally unlikely. Humans are constantly gesturing with their bodies, particularly in crowded social settings where a user would especially want to play music and interact with his/her music library. But over the course of a couple of group meetings during which we brainstormed, sketched, and toyed with the device, we settled on some gestures we all agreed would be effective to choose an item from a collection of items, which we would implement in the next phase of the project.			
			</p>

			<p>
Both of the interactions we designed involve a timeout to select the target, in order to avoid false positives, and one of them requires the user to first prime the device for his/her gestures by raising his/her other hand. Our decision to use these interactions was in part influenced by the Xbox interfaces, which we consider to be the current, most established "best practices" for the Kinect sensor. 
			</p>
		</a>


		<a name="implementation">
			<h1>
III. Implementation
			</h1>

			<p>
After the brainstorming and design phase of our project, we set up our development environments and divided the work to be done. Though none of us had experience with C#, the greatest hurdle of the implementation phase of our project was installing Windows 7, Visual Studio 2010, and the Kinect SDK. Many members of our group prefer to use Mac OS X, and these group members had to either partition their drives to install Boot Camp and Windows 7, obtain another computer with Windows 7 on it, or attempt to work from within a VM, which was strongly discouraged both in class and online.
			</p>

			<p>
Once we began coding, our project progressed swiftly. C#'s similarities to Java and the helpful class-provided starter code made it simple to learn to program for the Kinect. We were grateful for the Microsoft's Kinect SDK's built-in capability to track about 20 different joints (some more reliably than others). If we had gone another route and tried to use open-source Kinect SDK's for Mac OS X or Unix distributions, we wouldn't have had this luxury.
			</p>
		
			<iframe src="http://player.vimeo.com/video/35677971?title=0&amp;byline=0&amp;portrait=0" width="680" height="383" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>
			<span class="caption">
Keegan and Brad demonstrate the gestures we designed and implemented to select targets via the Kinect sensor
			</span>

			<p>Get our source code from <a href="https://github.com/nhippenmeyer/CS247" target="github">https://github.com/nhippenmeyer/CS247</a>, or <a href="https://github.com/nhippenmeyer/CS247/zipball/master">download a .zip directly</a>.</p>
		</a>


		<a name="discussion">
			<h1>
IV. Discussion
			</h1>

			<p>
The two gesture controls we designed and implemented each have unqiue advantages and disadvantages, lending each one to a different use case. The first one only involves waving a single arm, which places very little cognitive strain on the user; the connection between pointing and selecting is clear and intuitive. The second gesture control demands using both hands, which is both physically and cognitively more taxing on the user, but is less likely to trigger false positives. Our video (above) shows a simple use case where users might use our gestures to navigate their music library.
			</p>

			<p>
Using the gestures we designed and implemented to control the Kinect was an exciting experience that further convinced us of the great potential for natural user interfaes. Now that we've learned the ropes, and gotten our feet wet, we can't wait to get down and dirty with Kinect.
			</p>
		</a>

	</div>


	<footer>
		<nav>
			<a href="#introduction">
I. Introduction</a>
|
			<a href="#design_process">
II. Design Process</a>
|
			<a href="#implementation">
III. Implementation</a>
|
			<a href="#discussion">
IV. Discussion</a>
		</nav>

		<nav class="links">
			<a href="http://www.stanford.edu/" target="_">
Stanford University</a>
/
			<a href="http://hci.stanford.edu/courses/cs247/2012/" target="_">
Computer Science 247: Human-Computer Interaction Design Studio</a>
/
			<a href="http://hci.stanford.edu/courses/cs247/2012/p3.html" target="_">
Project 3: Gesture Controller</a>
		</nav>

Last modified January 26th, 2012
	</footer>

</body>
</html>

